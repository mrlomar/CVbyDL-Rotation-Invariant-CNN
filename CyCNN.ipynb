{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cylindrical CNN for Beer Cap Classification\n",
    "In this notebook, we try to develop a **Convolution Neural Network** that can classify randomly rotated beer caps while training on limited data. We do this by trying to make the network **invariant to rotation** to reduce the amount of samples needed for training. Our thought behind this is that a network that inherently comprehends rotation will need less training data to obtain reasonable results than a traditional CNN that needs to learn the rotations. To achieve this invariance, we implement the theory published in the paper [CyCNN: A Rotation Invariant CNN using Polar Mapping and Cylindrical Convolution Layers](https://arxiv.org/pdf/2007.10588.pdf). First we apply the theory to the MNIST dataset to assess whether it works as intended and secondly we apply the theory to our own generated data set to assess the viability of our approach.\n",
    "\n",
    "## Creating CyCNN\n",
    "Making the network invariant to rotations consists of two steps, namely: **(1)** Transforming the input to polar coordinates and **(2)** using Cylindrical Convolutional Layers. Both of these steps are discussed in their respective subsections.\n",
    "\n",
    "| Normal Coordinates | Polar Coordinates | Circular Padding |\n",
    "| :----------------: | :---------------: | :--------------: |\n",
    "| <img src=\"images/normal_rotations.gif\" width=\"400\"> | <img src=\"images/polar_rotations.gif\" width=\"400\"> | <img src=\"images/padded_rotations.gif\" width=\"400\"> |\n",
    "\n",
    "### Polar Coordinates\n",
    "The idea of polar coordinates is to express locations in the image as $(r, \\phi)$ instead of $(x, y)$, where $r$ is the distance to the origin and $\\phi$ is the angle. Because one of the coordinates includes the rotation in the original image, using polar coordinates transforms rotations in the original space to translations in polar space. Since CNN's *should* be translation invariant, this would in theory make the network invariant to rotations in the original space. But since there is no free lunch in computer science, using this does make the network less robust to translations in the original space.\n",
    "\n",
    "A visualisation of the process discussed in this subsection can be found in the first and second row of the table above.\n",
    "\n",
    "### Cylindrical Convolutional Layers\n",
    "The next step to making the CNN invariant to rotations is to implement Cylindrical Convolutions Layers. While this might sound intimidating, it is actually rather simple. To make the convolutional layer cylindrical, one only has to use a special type of padding. When using polar coordinates the $\\phi$ coordinate represents the rotation, this means that the top of the image connects to the bottom of the image. To communicate this information to the CNN the padding on the bottom should continue with the pixels found at the top of the image. For the $r$ coordinate there is no such relation. Because of this on the x-axis *Zero padding* is used.\n",
    "\n",
    "A visualisation of cylindrical padding can be found in the second and third row of the table above.\n",
    "\n",
    "## Beer Cap Dataset\n",
    "### Photo acquisition\n",
    "To create our beer cap dataset, a setup with a Raspberry Pi with Pi Camera was used. In this setup, a beer cap enters on top, when detected by a sensor, the first servo will let it through, then the beer cap enters a dark space which is lid by LEDs for consistent lighting conditions. There it is photographed, after which a second servo will let it pass and then it drops down in a big container. The first servo prevents a second beer cap from entering while another beer cap is in process. The setup also includes a 7-segment display that shows the total counted beer caps. We had some struggles with getting good lighting conditions since we wanted good and consistent photos. Photos taken in the middle of the day looked way better than later on the day. Furthermore, the direction of the light mattered for shadows a lot. We solved this by using LEDs and preventing most other light in the picture. Because the plexiglass in front of the beer caps reflects the LEDs light, this was also a challenge which we solve by careful placement of the LEDs, specific camera settings and accepting some reflections.\n",
    "\n",
    "|      |      |      |\n",
    "| :--: | :--: | :--: |\n",
    "| <img src=\"images/BCC.jpeg\" width=\"225\"> | <img src=\"images/BCC met bak.jpeg\" width=\"163\"> | <img src=\"images/beercap.png\" width=\"400\"> |\n",
    "\n",
    "### Data augmentation\n",
    "We wanted good and consistent images so we could increase the size of the dataset by data augmentation. We first cropped the pictures, then resized them to 32 by 32 pixels. We then included those images and several randomly rotated variants of those images in the dataset. The idea being that bottle caps that would enter already have a random rotation, and by randomly rotating images, we simulated more entries of the same brand.\n",
    "\n",
    "## Comparison MNIST\n",
    "### Network Architecture\n",
    "To evaluate our CyCNN on the MNIST dataset, the architecture seen below was used both for the CNN and CyCNN. Between each cube shown in the figure below there is a convolutional layer which applies the following steps to the input: **(1)** Apply cylindrical padding, **(2)** Apply a 3x3 convolution, **(3)** Apply the ReLu function and **(4)** Apply 2x2 max-pooling. The (Cy)CNN is then connected to a fully connected neural network to classify the input. When using the CyCNN model, there occurs a polar transformation between the input and the first convolution layer.\n",
    "![](images/MNIST_arch.png)\n",
    "\n",
    "### Experiment\n",
    "To assess the effectiveness of using the CyCNN rather than the traditional CNN, we compare the results of 4 models on non-rotated and randomly rotated images. The models we evaluate are the following:\n",
    "- **CyCNN<sup>R</sup>** - This model uses polar coordinates and cylindrical convolutional layers, but also includes an extra step. When an input is given to the network, CyCNN<sup>R</sup> first randomly rotates the image before feeding it to its learned network. This means that while training the model will see and learn rotations of the original data.\n",
    "\n",
    "- **CyCNN** - This model uses polar coordinates and cylindrical convolutional layers, but does not randomly rotate the images. This means that when the model is evaluated on rotated images, it will never have seen these rotations before. Any improvements over the traditional CNN will be purely due to the use of polar coordinates and cylindrical convolutional layers.\n",
    "\n",
    "- **CNN<sup>R</sup>** - This model is a standard CNN, but it also randomly rotated the input images before feeding it into the network. This approach could theoretically allow the CNN to learn the rotations on its own.\n",
    "\n",
    "- **CNN** - This model is a standard CNN that does not randomly rotate the images. Therefore, during testing it will never have seen these rotations before.\n",
    "\n",
    "To compare the models, each model is trained for 2 epochs. In the first epoch the learning rate is set to $0.001$ and in the second epoch the learning rate is set to $0.0001$.\n",
    "\n",
    "### Results\n",
    "The results of the experiment discussed above are shown in the table below.\n",
    "\n",
    "| Model | Normal Accuracy | Rotated Accuracy |\n",
    "| :---: | :-------------: | :--------------: |\n",
    "|CyCNN<sup>R</sup>|92%|**91%**|\n",
    "|CyCNN            |94%|72%|\n",
    "|CNN<sup>R</sup>  |88%|88%|\n",
    "|CNN              |**97%**|39%|\n",
    "\n",
    "As you can see, **CyCNN<sup>R</sup>** achieves the highest accuracy on the randomly rotated data. While the traditional CNN achieves the highest accuracy on the non-rotated data, its accuracy tanks when the images are randomly rotated (as expected). Comparing the CyCNN to the CNN, we can see that CyCNN inherently is more invariant to rotations, as it achieves an accuracy of 72% rather than 39% on the randomly rotated data. Overal, we can see that using a CyCNN<sup>R</sup> seems to be the best approach for our use case.\n",
    "\n",
    "Lastly, an interesting thing to note is the distinction between 6's and 9's when the data is randomly rotated. In theory, a network this is invariant to rotation should be able to distinguish a 6 from an upside-down 9 and vice-versa. A network that is not invariant to rotations would not be able to do this. To investigate this, we look at the prediction distribution for 6's and 9's for the CyCNN model. The results are shown below.\n",
    "\n",
    "| CyCNN 6 predictions | CyCNN 9 predictions |\n",
    "| :-----------------: | :------------------: | \n",
    "| <img src=\"images/sixes.png\" width=\"400\"> | <img src=\"images/nines.png\" width=\"400\"> |\n",
    "\n",
    "We can see that while the CyCNN is not able to distinguish between the two classes fully, it is able to correctly classify both classes the majority of the time. We can also see that strangely enough the network has more difficulty with distinguishing between a 9 and a 5 than between a 9 and a 6. This could be because when using polar coordinates a 5 might be more similar to a 9 than a 6 is to a 9.\n",
    "\n",
    "## Comparison Beer caps\n",
    "### Network Architecture\n",
    "To evaluate our CyCNN on the CAP dataset, the architecture seen below was used both for the CNN and CyCNN. Between each cube shown in the figure below there is a convolutional layer which applies the following steps to the input: **(1)** Apply cylindrical padding, **(2)** Apply a 3x3 convolution, **(3)** Apply the ReLu function and **(4)** Apply 2x2 max-pooling. The (Cy)CNN is then connected to a fully connected neural network to classify the input. When using the CyCNN model, there occurs a polar transformation between the input and the first convolution layer.\n",
    "![](images/CAP_arch.png)\n",
    "\n",
    "### Experiment (Marijn)\n",
    "Comparison for large amount of training data AND for smaller amount of training data.\n",
    "\n",
    "We vergelijken maar 2 modelen, CyCNN and CNN, aangezien de data al geroteerd is.\n",
    "\n",
    "### Results (Marijn)\n",
    "tabel met resultaten\n",
    "\n",
    "## Discussion (Samen, voor woensdag)\n",
    "Nog ff bedenken wat hier precies moet, mogelijk dingen in onze drive\n",
    "Instabiele training voor Beer caps, soms lijkt het trainen niet te lukken\n",
    "\n",
    "## Code\n",
    "Below, all the code for creating a CyCNN and running it on MNIST and our custom CAP dataset can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------\n",
    "# IMPORTS\n",
    "# -------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import WeightedRandomSampler, SubsetRandomSampler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import random, math\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameter\n",
    "NUM_CLASSES_CAP = 4\n",
    "\n",
    "# Seed for reproducability\n",
    "SEED = 42\n",
    "\n",
    "def set_seed():\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(SEED) + worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# BOTTLE CAP DATA AUGMENTATION\n",
    "# ----------------------------\n",
    "\n",
    "set_seed()\n",
    "\n",
    "SIZE = 32\n",
    "AUGMENT_DATA = False\n",
    "ROTATIONS = 4\n",
    "\n",
    "if AUGMENT_DATA:\n",
    "    cap_data = \"./data/CAPS_2/\"\n",
    "    new_cap_data = \"./data/CAPS\" + str(SIZE) + \"/\"\n",
    "    classes = range(NUM_CLASSES_CAP)\n",
    "\n",
    "    for c in classes:\n",
    "        new_dir = new_cap_data + str(c) + '/'\n",
    "        if not os.path.exists(new_dir):\n",
    "            os.makedirs(new_dir)\n",
    "        for filename in os.listdir(cap_data + str(c) + '/'):\n",
    "            if filename.endswith(\".png\"):\n",
    "                file = cap_data + str(c) + '/' + filename\n",
    "                img = cv.imread(file)\n",
    "                # Crop image to remove 32 pixels on the left and 16 pixels on the right, 24 top, 24 bottom\n",
    "                img = img[32:256-16, 24:256-24]\n",
    "                # Resize image\n",
    "                img = cv.resize(img, (SIZE, SIZE))\n",
    "                cv.imwrite(new_cap_data + str(c) + '/' + filename, img)\n",
    "                # save ROTATIONS rotations of the data\n",
    "                for r in range(ROTATIONS):\n",
    "                    M = cv.getRotationMatrix2D((SIZE // 2, SIZE // 2), random.uniform(0, 1) * 360, 1.0)\n",
    "                    rotated = cv.warpAffine(img, M, (SIZE, SIZE))\n",
    "                    cv.imwrite(new_cap_data + str(c) + '/r' + str(r) + '_' + filename, rotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------\n",
    "# FUNCTIONS RETRIEVED FROM https://github.com/mcrl/CyCNN/blob/master/cycnn/image_transforms.py\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "def polar_transform(images, transform_type='linearpolar'):\n",
    "    \"\"\"\n",
    "    This function takes multiple images, and apply polar coordinate conversion to it.\n",
    "    \"\"\"\n",
    "    results = images.clone()\n",
    "    \n",
    "    (N, C, H, W) = results.shape\n",
    "\n",
    "    for i in range(results.shape[0]):\n",
    "\n",
    "        img = results[i].numpy()  # [C,H,W]\n",
    "        img = np.transpose(img, (1, 2, 0))  # [H,W,C]\n",
    "\n",
    "        if transform_type == 'logpolar':\n",
    "            img = cv.logPolar(img, (H // 2, W // 2), W / math.log(W / 2), cv.WARP_FILL_OUTLIERS).reshape(H, W, C)\n",
    "        elif transform_type == 'linearpolar':\n",
    "            img = cv.linearPolar(img, (H // 2, W // 2), W / 2, cv.WARP_FILL_OUTLIERS).reshape(H, W, C)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "\n",
    "        results[i] = torch.from_numpy(img)\n",
    "\n",
    "    return results\n",
    "\n",
    "def random_rotate(images):\n",
    "    \"\"\"\n",
    "    This function takes multiple images, and rotate each image by a random angle [0, 360)\n",
    "    \"\"\"\n",
    "    new_images = []\n",
    "\n",
    "    (N, C, H, W) = images.shape\n",
    "\n",
    "    for i in range(images.shape[0]):\n",
    "        img = images[i].numpy()  # [C,H,W]\n",
    "\n",
    "        img = np.transpose(img, (1, 2, 0))  # [H,W,C]\n",
    "\n",
    "        d = random.randint(0, 360 - 1)\n",
    "\n",
    "        M = cv.getRotationMatrix2D((H // 2, W // 2), d, 1.0)\n",
    "        img_rot = cv.warpAffine(img, M, (H, W)).reshape(H, W, C)\n",
    "\n",
    "        img_rot = np.transpose(img_rot, (2, 0, 1))  # [C,H,W]\n",
    "        new_images.append(img_rot)\n",
    "\n",
    "    new_images = torch.from_numpy(np.stack(new_images, axis=0))\n",
    "\n",
    "    return new_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# DOWNLOADING AND LOADING MNIST DATASET\n",
    "# -------------------------------------\n",
    "\n",
    "data_dir='./data'\n",
    "\n",
    "MNIST_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Resize((32,32), torchvision.transforms.InterpolationMode.BILINEAR)\n",
    "    ])\n",
    "\n",
    "MNIST_train = torchvision.datasets.MNIST(root=data_dir, train=True, download=True, transform=MNIST_transform)\n",
    "MNIST_test = torchvision.datasets.MNIST(root=data_dir, train=False, download=True, transform=MNIST_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAP data loaded\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# LOADING THE CAPS DATASET\n",
    "# ------------------------\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Images should be normalized, which means we need to calculate/approximate the means and standard deviations\n",
    "APPROXIMATION_SAMPLES = 300\n",
    "CAP_DATA_FOLDER = \"./data/CAPS32/\"\n",
    "\n",
    "R = []\n",
    "G = []\n",
    "B = []\n",
    "\n",
    "for c in range(NUM_CLASSES_CAP):\n",
    "    sample = 0\n",
    "    for filename in os.listdir(CAP_DATA_FOLDER + str(c) + \"/\"):\n",
    "        if sample == APPROXIMATION_SAMPLES:\n",
    "            break\n",
    "        if filename.endswith(\".png\"):\n",
    "            file = CAP_DATA_FOLDER + str(c) + '/' + filename\n",
    "            img = cv.imread(file)\n",
    "            for x in img:\n",
    "                for pix in x:\n",
    "                    R.append(pix[0])\n",
    "                    G.append(pix[1])\n",
    "                    B.append(pix[2])\n",
    "            sample += 1\n",
    "            \n",
    "R = np.multiply(R, 1/255)\n",
    "G = np.multiply(G, 1/255)\n",
    "B = np.multiply(B, 1/255)\n",
    "\n",
    "R_avg = np.mean(R); R_std = np.std(R)\n",
    "G_avg = np.mean(G); G_std = np.std(G)\n",
    "B_avg = np.mean(B); B_std = np.std(B)\n",
    "\n",
    "CAP_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[R_avg, G_avg, B_avg],\n",
    "            std=[R_std, G_std, B_std],\n",
    "        ),\n",
    "        transforms.Resize((32,32), torchvision.transforms.InterpolationMode.BILINEAR)\n",
    "    ])\n",
    "\n",
    "\n",
    "CAP_data = torchvision.datasets.ImageFolder(root=CAP_DATA_FOLDER, transform=CAP_transform)\n",
    "\n",
    "print(\"CAP data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weightedSampler(data, classes):\n",
    "    data_size = len(data)\n",
    "    num_classes = len(classes)\n",
    "    class_weights = {}\n",
    "    for i in range(num_classes):\n",
    "        class_weights[classes[i]] = 0\n",
    "\n",
    "    for i in range(data_size):\n",
    "        img, label = data[i]\n",
    "        class_weights[classes[label]] += 1\n",
    "        \n",
    "    min_num_samples = sys.maxsize\n",
    "    \n",
    "    for label, count in class_weights.items():\n",
    "        min_num_samples = min(min_num_samples, count)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        class_weights[classes[i]] = min_num_samples / class_weights[classes[i]]\n",
    "\n",
    "    sample_weights = [0] * data_size\n",
    "\n",
    "\n",
    "    for idx, (_, label) in enumerate(data):\n",
    "        sample_weights[idx] = class_weights[str(label)]\n",
    "    \n",
    "    g_cpu = torch.Generator()\n",
    "    g_cpu.manual_seed(SEED)\n",
    "\n",
    "    return WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True, generator=g_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset, train_proportion=0.8, test_proportion=0.2):\n",
    "    '''\n",
    "    Returns a train- and test-loader based on the name of the data set\n",
    "    The option for `dataset` are: `MNIST` and `CAP`\n",
    "    '''\n",
    "    if dataset == 'MNIST':\n",
    "        return torch.utils.data.DataLoader(MNIST_train), torch.utils.data.DataLoader(MNIST_test)\n",
    "    if dataset == 'CAP':\n",
    "        \n",
    "        data_size = len(CAP_data)\n",
    "        test_size = round(test_proportion * data_size)\n",
    "        train_size = min(data_size - test_size, round(train_proportion * data_size))\n",
    "        remaining_size = data_size - test_size - train_size\n",
    "\n",
    "        CAP_data_train, CAP_data_test, _ = torch.utils.data.random_split(CAP_data, [train_size, test_size, remaining_size])\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(CAP_data_train, sampler=get_weightedSampler(CAP_data_train, CAP_data.classes), batch_size=1,num_workers=0, worker_init_fn=_init_fn)\n",
    "        test_loader = torch.utils.data.DataLoader(CAP_data_test, batch_size=1,num_workers=0, worker_init_fn=_init_fn)\n",
    "        return train_loader, test_loader\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 5770 {'0': 3120, '1': 1800, '2': 520, '3': 330}\n",
      "train 4616 {'0': 1160, '1': 1127, '2': 1175, '3': 1154}\n",
      "test 1154 {'0': 605, '1': 364, '2': 107, '3': 78}\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# CHECK TRAIN TEST SPLIT AND CLASS BALANCING\n",
    "# ------------------------------------------\n",
    "\n",
    "set_seed()\n",
    "\n",
    "train, test = load_data(\"CAP\")\n",
    "data = torch.utils.data.DataLoader(CAP_data, batch_size=1)\n",
    "label_counts = {}\n",
    "\n",
    "\n",
    "for label in CAP_data.classes:\n",
    "    label_counts[label] = 0\n",
    "    \n",
    "for _, labels in data:\n",
    "    for idx, label in enumerate(CAP_data.classes):\n",
    "        label_counts[label] += torch.sum(labels==idx).item()\n",
    "\n",
    "print(\"data\", len(CAP_data), label_counts)\n",
    "\n",
    "\n",
    "for label in CAP_data.classes:\n",
    "    label_counts[label] = 0\n",
    "    \n",
    "for _, labels in train:\n",
    "    for idx, label in enumerate(CAP_data.classes):\n",
    "        label_counts[label] += torch.sum(labels==idx).item()\n",
    "\n",
    "print(\"train\", len(train), label_counts)\n",
    "\n",
    "for label in CAP_data.classes:\n",
    "    label_counts[label] = 0\n",
    "    \n",
    "for _, labels in test:\n",
    "    for idx, label in enumerate(CAP_data.classes):\n",
    "        label_counts[label] += torch.sum(labels==idx).item()\n",
    "    \n",
    "print(\"test\", len(test), label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data label: tensor(7) \n",
      "Original Data\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original => Polar\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotated Original\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotated Original => Polar\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cyclic Padding\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# EXAMPLE OF POLAR TRANSFORMATION AND PADDING\n",
    "# -------------------------------------------\n",
    "\n",
    "_, example_data = load_data('MNIST')\n",
    "example_iter = iter(example_data)\n",
    "\n",
    "for i in range(1):\n",
    "    image, label = example_iter.next()\n",
    "    print('Data label:', label[0], '\\nOriginal Data')\n",
    "    plt.imshow(np.transpose(image[0].numpy(), (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "    print('Original => Polar')\n",
    "    new_image = image.clone()\n",
    "    new_image = polar_transform(new_image)\n",
    "    plt.imshow(np.transpose(new_image[0].numpy(), (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "    print('Rotated Original')\n",
    "    image = random_rotate(image)\n",
    "    plt.imshow(np.transpose(image[0].numpy(), (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "    print('Rotated Original => Polar')\n",
    "    new_image = image.clone()\n",
    "    new_image = polar_transform(new_image)\n",
    "    plt.imshow(np.transpose(new_image[0].numpy(), (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "    print('Cyclic Padding')\n",
    "    N = 5\n",
    "    new_image = F.pad(new_image, (0, 0, N, N), mode='circular')\n",
    "    new_image = F.pad(new_image, (N, N, 0, 0), mode='constant')\n",
    "    plt.imshow(np.transpose(new_image[0].numpy(), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------\n",
    "# DEFINE CyCNN\n",
    "# ------------\n",
    "\n",
    "set_seed()\n",
    "\n",
    "class CyNet(nn.Module):\n",
    "    def __init__(self, layers=None, data='MNIST', random_rotate=False):\n",
    "        '''\n",
    "        `layers` can be passed to create a 'custom' CyCNN.\n",
    "        layers should be a tuple (C, L), with:\n",
    "            C => a list containing a triplet (INPUT_CHANNELS, OUTPUT_CHANNELS, KERNEL_SIZE) for each convolutional layer\n",
    "            L => a list containing a tuple (INPUT_CHANNELS, OUTPUT_CHANNELS) for each linear layer\n",
    "        \n",
    "        `data` can be passed to indicate which default network to use.\n",
    "        \n",
    "        `random_rotate` can be used to make the network generalize better.\n",
    "        However, there are some data requirements for it to work, which is why it is disabled by default.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create Cyclic padding\n",
    "        self.padVer = lambda x: F.pad(x, (0, 0, 1, 1), mode='circular')\n",
    "        self.padHor = lambda x: F.pad(x, (1, 1, 0, 0), mode='constant')\n",
    "\n",
    "        # Define the pooling used\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Create lists used for the convolutional- and linear layers.\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.fcs = nn.ModuleList()\n",
    "        self.view = -1\n",
    "        \n",
    "        self.random_rotate = random_rotate\n",
    "        \n",
    "        if layers is None:\n",
    "            # If no layers are passed, create the default network\n",
    "            # The default network is dependent on the data set\n",
    "            if data == 'MNIST':\n",
    "                # Default network for MNIST dataset\n",
    "                self.convs.append(nn.Conv2d(1, 3, 3))\n",
    "                self.convs.append(nn.Conv2d(3, 8, 3))\n",
    "                self.convs.append(nn.Conv2d(8, 16, 3))\n",
    "                self.convs.append(nn.Conv2d(16, 32, 3))\n",
    "                self.convs.append(nn.Conv2d(32, 64, 3))\n",
    "\n",
    "                self.view = 64\n",
    "\n",
    "                self.fcs.append(nn.Linear(64, 32))\n",
    "                self.fcs.append(nn.Linear(32, 24))\n",
    "                self.fcs.append(nn.Linear(24, 10))\n",
    "\n",
    "            if data == 'CAP':\n",
    "                # Default network for CAP dataset\n",
    "                self.convs.append(nn.Conv2d(3, 4, 3))\n",
    "                self.convs.append(nn.Conv2d(4, 6, 3))\n",
    "                self.convs.append(nn.Conv2d(6, 12, 3))\n",
    "                self.convs.append(nn.Conv2d(12, 16, 3))\n",
    "                self.convs.append(nn.Conv2d(16, 24, 3))\n",
    "\n",
    "                self.view = 24\n",
    "\n",
    "                self.fcs.append(nn.Linear(24, 12))\n",
    "                self.fcs.append(nn.Linear(12, NUM_CLASSES_CAP))\n",
    "                \n",
    "                #self.convs.append(nn.Conv2d(3, 8, 3))\n",
    "                #self.convs.append(nn.Conv2d(8, 16, 3))\n",
    "                #self.convs.append(nn.Conv2d(16, 24, 3))\n",
    "                #self.convs.append(nn.Conv2d(24, 32, 3))\n",
    "                #self.convs.append(nn.Conv2d(32, 64, 3))\n",
    "\n",
    "                #self.view = 64\n",
    "\n",
    "                #self.fcs.append(nn.Linear(64, 32))\n",
    "                #self.fcs.append(nn.Linear(32, 16))\n",
    "                #self.fcs.append(nn.Linear(16, 8))\n",
    "        else:\n",
    "            # Parse the given layers\n",
    "            for cl in layers[0]:\n",
    "                self.convs.append(nn.Conv2d(cl[0], cl[1], cl[2]))\n",
    "            for fcl in layers[1]:\n",
    "                self.fcs.append(nn.Linear(fcl[0], fcl[1]))\n",
    "            self.view = layers[1][0][0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.random_rotate:\n",
    "            # Randomly rotate the input data is the option is enabled.\n",
    "            x = random_rotate(x)\n",
    "        # Perform polar transformation on the input\n",
    "        x = polar_transform(x)\n",
    "        # Run the sample through all convolutional layers\n",
    "        for conv in self.convs:\n",
    "            x = self.pool(F.relu(conv(self.padVer(self.padHor(x)))))\n",
    "        # Run the result from the convolution through the linear layers\n",
    "        x = x.view(-1, self.view)\n",
    "        for l in self.fcs[:-1]:\n",
    "            x = F.relu(l(x))\n",
    "        # Don't use the relu for the last layer\n",
    "        x = self.fcs[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# DEFINE EQUIVALENT CNN FOR COMPARISON\n",
    "# ------------------------------------\n",
    "\n",
    "set_seed()\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, layers=None, data='MNIST', random_rotate=False):\n",
    "        '''\n",
    "        `layers` can be passed to create a 'custom' CNN.\n",
    "        layers should be a tuple (C, L), with:\n",
    "            C => a list containing a triplet (INPUT_CHANNELS, OUTPUT_CHANNELS, KERNEL_SIZE) for each convolutional layer\n",
    "            L => a list containing a tuple (INPUT_CHANNELS, OUTPUT_CHANNELS) for each linear layer\n",
    "        \n",
    "        `data` can be passed to indicate which default network to use.\n",
    "        \n",
    "        `random_rotate` can be used to make the network generalize better.\n",
    "        However, there are some data requirements for it to work, which is why it is disabled by default.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create zero padding\n",
    "        self.padVer = lambda x: F.pad(x, (0, 0, 1, 1), mode='constant')\n",
    "        self.padHor = lambda x: F.pad(x, (1, 1, 0, 0), mode='constant')\n",
    "        \n",
    "        # Define the pooling used\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Create lists used for the convolutional- and linear layers.\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.fcs = nn.ModuleList()\n",
    "        self.view = -1\n",
    "        \n",
    "        self.random_rotate = random_rotate\n",
    "        \n",
    "        if layers is None:\n",
    "            # If no layers are passed, create the default network\n",
    "            # The default network is dependent on the data set\n",
    "            if data == 'MNIST':\n",
    "                # Default network for MNIST dataset\n",
    "                self.convs.append(nn.Conv2d(1, 3, 3))\n",
    "                self.convs.append(nn.Conv2d(3, 8, 3))\n",
    "                self.convs.append(nn.Conv2d(8, 16, 3))\n",
    "                self.convs.append(nn.Conv2d(16, 32, 3))\n",
    "                self.convs.append(nn.Conv2d(32, 64, 3))\n",
    "\n",
    "                self.view = 64\n",
    "\n",
    "                self.fcs.append(nn.Linear(64, 32))\n",
    "                self.fcs.append(nn.Linear(32, 24))\n",
    "                self.fcs.append(nn.Linear(24, 10))\n",
    "\n",
    "            if data == 'CAP':\n",
    "                # Default network for CAP dataset\n",
    "                self.convs.append(nn.Conv2d(3, 4, 3))\n",
    "                self.convs.append(nn.Conv2d(4, 6, 3))\n",
    "                self.convs.append(nn.Conv2d(6, 12, 3))\n",
    "                self.convs.append(nn.Conv2d(12, 16, 3))\n",
    "                self.convs.append(nn.Conv2d(16, 24, 3))\n",
    "\n",
    "                self.view = 24\n",
    "\n",
    "                self.fcs.append(nn.Linear(24, 12))\n",
    "                self.fcs.append(nn.Linear(12, NUM_CLASSES_CAP))\n",
    "        else:\n",
    "            # Parse the given layers\n",
    "            for cl in layers[0]:\n",
    "                self.convs.append(nn.Conv2d(cl[0], cl[1], cl[2]))\n",
    "            for fcl in layers[1]:\n",
    "                self.fcs.append(nn.Linear(fcl[0], fcl[1]))\n",
    "            self.view = layers[1][0][0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.random_rotate:\n",
    "            # Randomly rotate the input data is the option is enabled.\n",
    "            x = random_rotate(x)\n",
    "        # Run the sample through all convolutional layers\n",
    "        for conv in self.convs:\n",
    "            x = self.pool(F.relu(conv(self.padVer(self.padHor(x)))))\n",
    "        # Run the result from the convolution through the linear layers\n",
    "        x = x.view(-1, self.view)\n",
    "        for l in self.fcs[:-1]:\n",
    "            x = F.relu(l(x))\n",
    "        # Don't use the relu for the last layer\n",
    "        x = self.fcs[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs, lrs=[(0, 0.001), (1, 0.0001)], momentum=0.9, print_interval=2000, train_proportion=0.8):\n",
    "    set_seed()\n",
    "    \n",
    "    train_loader, _ = load_data(data, train_proportion)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=momentum)\n",
    "    \n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = \"cpu\"\n",
    "    net.to(device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"Started at: \" + time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "    \n",
    "    for epoch in range(epochs):  \n",
    "        print('== Starting epoch ' + str(epoch) + ' ==')\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for e, lr in lrs:\n",
    "            if epoch == e:\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] = lr\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % print_interval == print_interval - 1:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / print_interval))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Finished Training in: ' + str(timedelta(seconds=elapsed_time)))\n",
    "\n",
    "def test(net, data, test_proportion=0.2):\n",
    "    set_seed()\n",
    "    \n",
    "    _, test_loader = load_data(data, test_proportion)\n",
    "    device = \"cpu\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the', len(test_loader), 'test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(random_rotate(images))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the', len(test_loader), 'randomly rotated test images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycnn_R = CyNet(data='MNIST', random_rotate=True)\n",
    "train(cycnn_R, 'MNIST', 2, lrs=[(0, 0.001), (1, 0.0001)])\n",
    "\n",
    "cycnn = CyNet(data='MNIST', random_rotate=False)\n",
    "train(cycnn, 'MNIST', 2, lrs=[(0, 0.001), (1, 0.0001)])\n",
    "\n",
    "cnn_R = CNN(data='MNIST', random_rotate=True)\n",
    "train(cnn_R, 'MNIST', 2, lrs=[(0, 0.001), (1, 0.0001)])\n",
    "\n",
    "cnn = CNN(data='MNIST', random_rotate=False)\n",
    "train(cnn, 'MNIST', 2, lrs=[(0, 0.001), (1, 0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('== CyCNN Rotated Training ==')\n",
    "test(cycnn_R, 'MNIST')\n",
    "\n",
    "print('== CyCNN Normal Training ==')\n",
    "test(cycnn, 'MNIST')\n",
    "\n",
    "print('== CNN Rotated Training ==')\n",
    "test(cnn_R, 'MNIST')\n",
    "\n",
    "print('== CNN Normal Training ==')\n",
    "test(cnn, 'MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 16:41:39\n",
      "== Starting epoch 0 ==\n",
      "[1,   500] loss: 1.387\n",
      "[1,  1000] loss: 1.386\n",
      "[1,  1500] loss: 1.386\n",
      "[1,  2000] loss: 1.308\n",
      "[1,  2500] loss: 1.165\n",
      "[1,  3000] loss: 1.108\n",
      "[1,  3500] loss: 1.012\n",
      "[1,  4000] loss: 0.913\n",
      "[1,  4500] loss: 0.816\n",
      "== Starting epoch 1 ==\n",
      "[2,   500] loss: 0.774\n",
      "[2,  1000] loss: 0.635\n",
      "[2,  1500] loss: 0.960\n",
      "[2,  2000] loss: 0.916\n",
      "[2,  2500] loss: 0.804\n",
      "[2,  3000] loss: 0.714\n",
      "[2,  3500] loss: 0.609\n",
      "[2,  4000] loss: 0.578\n",
      "[2,  4500] loss: 0.459\n",
      "== Starting epoch 2 ==\n",
      "[3,   500] loss: 0.437\n",
      "[3,  1000] loss: 0.403\n",
      "[3,  1500] loss: 0.422\n",
      "[3,  2000] loss: 0.404\n",
      "[3,  2500] loss: 0.365\n",
      "[3,  3000] loss: 0.353\n",
      "[3,  3500] loss: 0.339\n",
      "[3,  4000] loss: 0.302\n",
      "[3,  4500] loss: 0.250\n",
      "== Starting epoch 3 ==\n",
      "[4,   500] loss: 0.304\n",
      "[4,  1000] loss: 0.219\n",
      "[4,  1500] loss: 0.175\n",
      "[4,  2000] loss: 0.244\n",
      "[4,  2500] loss: 0.228\n",
      "[4,  3000] loss: 0.185\n",
      "[4,  3500] loss: 0.187\n",
      "[4,  4000] loss: 0.179\n",
      "[4,  4500] loss: 0.161\n",
      "== Starting epoch 4 ==\n",
      "[5,   500] loss: 0.135\n",
      "[5,  1000] loss: 0.188\n",
      "[5,  1500] loss: 0.180\n",
      "[5,  2000] loss: 0.140\n",
      "[5,  2500] loss: 0.486\n",
      "[5,  3000] loss: 0.896\n",
      "[5,  3500] loss: 0.529\n",
      "[5,  4000] loss: 0.225\n",
      "[5,  4500] loss: 0.278\n",
      "== Starting epoch 5 ==\n",
      "[6,   500] loss: 0.160\n",
      "[6,  1000] loss: 0.111\n",
      "[6,  1500] loss: 0.133\n",
      "[6,  2000] loss: 0.096\n",
      "[6,  2500] loss: 0.071\n",
      "[6,  3000] loss: 0.098\n",
      "[6,  3500] loss: 0.088\n",
      "[6,  4000] loss: 0.077\n",
      "[6,  4500] loss: 0.087\n",
      "== Starting epoch 6 ==\n",
      "[7,   500] loss: 0.090\n",
      "[7,  1000] loss: 0.060\n",
      "[7,  1500] loss: 0.082\n",
      "[7,  2000] loss: 0.077\n",
      "[7,  2500] loss: 0.095\n",
      "[7,  3000] loss: 0.064\n",
      "[7,  3500] loss: 0.076\n",
      "[7,  4000] loss: 0.082\n",
      "[7,  4500] loss: 0.060\n",
      "== Starting epoch 7 ==\n",
      "[8,   500] loss: 0.069\n",
      "[8,  1000] loss: 0.089\n",
      "[8,  1500] loss: 0.070\n",
      "[8,  2000] loss: 0.094\n",
      "[8,  2500] loss: 0.069\n",
      "[8,  3000] loss: 0.057\n",
      "[8,  3500] loss: 0.046\n",
      "[8,  4000] loss: 0.053\n",
      "[8,  4500] loss: 0.072\n",
      "== Starting epoch 8 ==\n",
      "[9,   500] loss: 0.061\n",
      "[9,  1000] loss: 0.075\n",
      "[9,  1500] loss: 0.052\n",
      "[9,  2000] loss: 0.038\n",
      "[9,  2500] loss: 0.080\n",
      "[9,  3000] loss: 0.060\n",
      "[9,  3500] loss: 0.051\n",
      "[9,  4000] loss: 0.040\n",
      "[9,  4500] loss: 0.041\n",
      "== Starting epoch 9 ==\n",
      "[10,   500] loss: 0.067\n",
      "[10,  1000] loss: 0.086\n",
      "[10,  1500] loss: 0.080\n",
      "[10,  2000] loss: 0.026\n",
      "[10,  2500] loss: 0.068\n",
      "[10,  3000] loss: 0.054\n",
      "[10,  3500] loss: 0.043\n",
      "[10,  4000] loss: 0.070\n",
      "[10,  4500] loss: 0.024\n",
      "Finished Training\n",
      "Finished Training in: 0:02:40.956303\n",
      "Started at: 16:44:23\n",
      "== Starting epoch 0 ==\n",
      "[1,   500] loss: 1.388\n",
      "[1,  1000] loss: 1.387\n",
      "[1,  1500] loss: 1.390\n",
      "[1,  2000] loss: 1.389\n",
      "[1,  2500] loss: 1.384\n",
      "[1,  3000] loss: 1.389\n",
      "[1,  3500] loss: 1.373\n",
      "[1,  4000] loss: 1.302\n",
      "[1,  4500] loss: 1.087\n",
      "== Starting epoch 1 ==\n",
      "[2,   500] loss: 0.862\n",
      "[2,  1000] loss: 0.695\n",
      "[2,  1500] loss: 0.591\n",
      "[2,  2000] loss: 0.457\n",
      "[2,  2500] loss: 0.431\n",
      "[2,  3000] loss: 0.434\n",
      "[2,  3500] loss: 0.383\n",
      "[2,  4000] loss: 0.925\n",
      "[2,  4500] loss: 1.125\n",
      "== Starting epoch 2 ==\n",
      "[3,   500] loss: 0.896\n",
      "[3,  1000] loss: 0.891\n",
      "[3,  1500] loss: 0.848\n",
      "[3,  2000] loss: 0.798\n",
      "[3,  2500] loss: 0.819\n",
      "[3,  3000] loss: 0.622\n",
      "[3,  3500] loss: 0.469\n",
      "[3,  4000] loss: 0.427\n",
      "[3,  4500] loss: 0.403\n",
      "== Starting epoch 3 ==\n",
      "[4,   500] loss: 0.346\n",
      "[4,  1000] loss: 0.329\n",
      "[4,  1500] loss: 0.392\n",
      "[4,  2000] loss: 0.331\n",
      "[4,  2500] loss: 0.313\n",
      "[4,  3000] loss: 0.293\n",
      "[4,  3500] loss: 0.432\n",
      "[4,  4000] loss: 0.276\n",
      "[4,  4500] loss: 0.300\n",
      "== Starting epoch 4 ==\n",
      "[5,   500] loss: 0.213\n",
      "[5,  1000] loss: 0.165\n",
      "[5,  1500] loss: 0.222\n",
      "[5,  2000] loss: 0.193\n",
      "[5,  2500] loss: 0.174\n",
      "[5,  3000] loss: 0.146\n",
      "[5,  3500] loss: 0.161\n",
      "[5,  4000] loss: 0.131\n",
      "[5,  4500] loss: 0.172\n",
      "== Starting epoch 5 ==\n",
      "[6,   500] loss: 0.102\n",
      "[6,  1000] loss: 0.113\n",
      "[6,  1500] loss: 0.093\n",
      "[6,  2000] loss: 0.086\n",
      "[6,  2500] loss: 0.057\n",
      "[6,  3000] loss: 0.087\n",
      "[6,  3500] loss: 0.085\n",
      "[6,  4000] loss: 0.073\n",
      "[6,  4500] loss: 0.073\n",
      "== Starting epoch 6 ==\n",
      "[7,   500] loss: 0.071\n",
      "[7,  1000] loss: 0.062\n",
      "[7,  1500] loss: 0.085\n",
      "[7,  2000] loss: 0.057\n",
      "[7,  2500] loss: 0.082\n",
      "[7,  3000] loss: 0.056\n",
      "[7,  3500] loss: 0.058\n",
      "[7,  4000] loss: 0.043\n",
      "[7,  4500] loss: 0.056\n",
      "== Starting epoch 7 ==\n",
      "[8,   500] loss: 0.049\n",
      "[8,  1000] loss: 0.055\n",
      "[8,  1500] loss: 0.064\n",
      "[8,  2000] loss: 0.041\n",
      "[8,  2500] loss: 0.070\n",
      "[8,  3000] loss: 0.041\n",
      "[8,  3500] loss: 0.064\n",
      "[8,  4000] loss: 0.072\n",
      "[8,  4500] loss: 0.049\n",
      "== Starting epoch 8 ==\n",
      "[9,   500] loss: 0.040\n",
      "[9,  1000] loss: 0.038\n",
      "[9,  1500] loss: 0.051\n",
      "[9,  2000] loss: 0.044\n",
      "[9,  2500] loss: 0.054\n",
      "[9,  3000] loss: 0.032\n",
      "[9,  3500] loss: 0.038\n",
      "[9,  4000] loss: 0.035\n",
      "[9,  4500] loss: 0.039\n",
      "== Starting epoch 9 ==\n",
      "[10,   500] loss: 0.040\n",
      "[10,  1000] loss: 0.037\n",
      "[10,  1500] loss: 0.042\n",
      "[10,  2000] loss: 0.024\n",
      "[10,  2500] loss: 0.061\n",
      "[10,  3000] loss: 0.034\n",
      "[10,  3500] loss: 0.039\n",
      "[10,  4000] loss: 0.045\n",
      "[10,  4500] loss: 0.039\n",
      "Finished Training\n",
      "Finished Training in: 0:02:40.081285\n",
      "Started at: 16:47:06\n",
      "== Starting epoch 0 ==\n",
      "[1,   500] loss: 1.388\n",
      "[1,  1000] loss: 1.387\n",
      "[1,  1500] loss: 1.391\n",
      "[1,  2000] loss: 1.389\n",
      "[1,  2500] loss: 1.385\n",
      "[1,  3000] loss: 1.392\n",
      "[1,  3500] loss: 1.387\n",
      "[1,  4000] loss: 1.387\n",
      "[1,  4500] loss: 1.384\n",
      "== Starting epoch 1 ==\n",
      "[2,   500] loss: 1.383\n",
      "[2,  1000] loss: 1.336\n",
      "[2,  1500] loss: 1.352\n",
      "[2,  2000] loss: 1.383\n",
      "[2,  2500] loss: 1.389\n",
      "[2,  3000] loss: 1.388\n",
      "[2,  3500] loss: 1.386\n",
      "[2,  4000] loss: 1.390\n",
      "[2,  4500] loss: 1.389\n",
      "== Starting epoch 2 ==\n",
      "[3,   500] loss: 1.389\n",
      "[3,  1000] loss: 1.387\n",
      "[3,  1500] loss: 1.388\n",
      "[3,  2000] loss: 1.387\n",
      "[3,  2500] loss: 1.389\n",
      "[3,  3000] loss: 1.389\n",
      "[3,  3500] loss: 1.389\n",
      "[3,  4000] loss: 1.390\n",
      "[3,  4500] loss: 1.385\n",
      "== Starting epoch 3 ==\n",
      "[4,   500] loss: 1.389\n",
      "[4,  1000] loss: 1.389\n",
      "[4,  1500] loss: 1.388\n",
      "[4,  2000] loss: 1.387\n",
      "[4,  2500] loss: 1.391\n",
      "[4,  3000] loss: 1.388\n",
      "[4,  3500] loss: 1.389\n",
      "[4,  4000] loss: 1.390\n",
      "[4,  4500] loss: 1.386\n",
      "== Starting epoch 4 ==\n",
      "[5,   500] loss: 1.385\n",
      "[5,  1000] loss: 1.385\n",
      "[5,  1500] loss: 1.388\n",
      "[5,  2000] loss: 1.389\n",
      "[5,  2500] loss: 1.388\n",
      "[5,  3000] loss: 1.388\n",
      "[5,  3500] loss: 1.388\n",
      "[5,  4000] loss: 1.387\n",
      "[5,  4500] loss: 1.390\n",
      "== Starting epoch 5 ==\n",
      "[6,   500] loss: 1.388\n",
      "[6,  1000] loss: 1.386\n",
      "[6,  1500] loss: 1.386\n",
      "[6,  2000] loss: 1.387\n",
      "[6,  2500] loss: 1.386\n",
      "[6,  3000] loss: 1.386\n",
      "[6,  3500] loss: 1.387\n",
      "[6,  4000] loss: 1.387\n",
      "[6,  4500] loss: 1.387\n",
      "== Starting epoch 6 ==\n",
      "[7,   500] loss: 1.387\n",
      "[7,  1000] loss: 1.387\n",
      "[7,  1500] loss: 1.387\n",
      "[7,  2000] loss: 1.386\n",
      "[7,  2500] loss: 1.387\n",
      "[7,  3000] loss: 1.386\n",
      "[7,  3500] loss: 1.387\n",
      "[7,  4000] loss: 1.386\n",
      "[7,  4500] loss: 1.385\n",
      "== Starting epoch 7 ==\n",
      "[8,   500] loss: 1.387\n",
      "[8,  1000] loss: 1.386\n",
      "[8,  1500] loss: 1.386\n",
      "[8,  2000] loss: 1.387\n",
      "[8,  2500] loss: 1.386\n",
      "[8,  3000] loss: 1.386\n",
      "[8,  3500] loss: 1.386\n",
      "[8,  4000] loss: 1.387\n",
      "[8,  4500] loss: 1.387\n",
      "== Starting epoch 8 ==\n",
      "[9,   500] loss: 1.385\n",
      "[9,  1000] loss: 1.386\n",
      "[9,  1500] loss: 1.388\n",
      "[9,  2000] loss: 1.386\n",
      "[9,  2500] loss: 1.388\n",
      "[9,  3000] loss: 1.386\n",
      "[9,  3500] loss: 1.386\n",
      "[9,  4000] loss: 1.387\n",
      "[9,  4500] loss: 1.387\n",
      "== Starting epoch 9 ==\n",
      "[10,   500] loss: 1.385\n",
      "[10,  1000] loss: 1.387\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# TEST CNN AND CYCNN ON FULL BEER CAP DATASET\n",
    "# -------------------------------------------\n",
    "\n",
    "cycnn_R = CyNet(data='CAP', random_rotate=True)\n",
    "train(cycnn_R, 'CAP', 10, lrs=[(0, 0.001), (5, 0.0001)], momentum=0.9, print_interval=500)\n",
    "\n",
    "cycnn = CyNet(data='CAP', random_rotate=False)\n",
    "train(cycnn, 'CAP', 10, lrs=[(0, 0.001), (5, 0.0001)], momentum=0.9, print_interval=500)\n",
    "\n",
    "cnn_R = CNN(data='CAP', random_rotate=True)\n",
    "train(cnn_R, 'CAP', 10, lrs=[(0, 0.001), (5, 0.0001)], momentum=0.9, print_interval=500)\n",
    "\n",
    "cnn = CNN(data='CAP', random_rotate=False)\n",
    "train(cnn, 'CAP', 10, lrs=[(0, 0.001), (5, 0.0001)], momentum=0.9, print_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('== CyCNN Rotated Training ==')\n",
    "test(cycnn_R, 'CAP')\n",
    "\n",
    "print('== CyCNN Normal Training ==')\n",
    "test(cycnn, 'CAP')\n",
    "\n",
    "print('== CNN Rotated Training ==')\n",
    "test(cnn_R, 'CAP')\n",
    "\n",
    "print('== CNN Normal Training ==')\n",
    "test(cnn, 'CAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_usage = [0.8, 0.6, 0.4, 0.2]\n",
    "\n",
    "\n",
    "for data_proportion in data_usage:\n",
    "\n",
    "    cycnn = CyNet(data='CAP', random_rotate=False)\n",
    "    train(cycnn, 'CAP', 10, lrs=[(0, 0.001), (5, 0.0001)], momentum=0.9, print_interval=500, train_proportion=data_proportion)\n",
    "    test(cycnn, 'CAP')\n",
    "    \n",
    "    cnn = CNN(data='CAP', random_rotate=False)\n",
    "    train(cnn, 'CAP', 10, lrs=[(0, 0.001), (5, 0.0001)], momentum=0.9, print_interval=500, train_proportion=data_proportion)\n",
    "    test(cnn, 'CAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
